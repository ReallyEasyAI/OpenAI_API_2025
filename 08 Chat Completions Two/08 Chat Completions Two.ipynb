{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Completions Two\n",
    "\n",
    "## Temperature\n",
    "\n",
    "### Controlling Creativity with the Temperature Parameter\n",
    "\n",
    "In this example, we introduce the `temperature` parameter to control the randomness and creativity of the model's responses. A lower temperature (e.g., `0`) produces deterministic, predictable outputs suitable for clear and consistent writing. A higher temperature (closer to `1`) yields more creative and varied responses.\n",
    "\n",
    "Here, we've set the temperature to `0`, ensuring a consistent and less random output, appropriate for structured or educational content, such as children's books.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script shows how to use the OpenAI API to generate text completions.\n",
    "We add the temperature parameter to specify the randomness/creativity of the response.\n",
    "In this case, we want the response to be less random/creative.\n",
    "\"\"\"\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"developer\", \"content\": \"You are a brilliant author of children's books.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write two paragraphs about a frog.\"}\n",
    "  ],\n",
    "  response_format=None,  \n",
    "  temperature=0 # Lower temperature for more deterministic output\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increasing Creativity with a Higher Temperature\n",
    "\n",
    "Here, we use a higher `temperature` parameter (`1.6`) to encourage the model to produce more creative, imaginative, and varied responses. A higher temperature is ideal when you want the output to be playful, diverse, or surprisingâ€”perfect for storytelling or creative writing tasks.\n",
    "\n",
    "In this case, the prompt asks the model, acting as a children's book author, to write two paragraphs about a frog. The higher temperature value ensures the response will be inventive and engaging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script shows how to use the OpenAI API to generate text completions.\n",
    "We add the temperature parameter to specify the randomness/creativity of the response.\n",
    "In this case, we want the response to be more random/creative.\n",
    "\"\"\"\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"developer\", \"content\": \"You are a brilliant author of children's books.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write two paragraphs about a frog.\"}\n",
    "  ],\n",
    "  response_format=None,  \n",
    "  temperature=1.6 # Higher temperature for less deterministic output\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Completion Tokens\n",
    "\n",
    "### Limiting Response Length with `max_completion_tokens`\n",
    "\n",
    "In this example, we demonstrate the use of the `max_completion_tokens` parameter, which limits the length of the generated text response. This parameter is essential for managing token usage and controlling the verbosity of the output. Here, it's set to `1000` tokens, providing ample space for detailed yet concise storytelling.\n",
    "\n",
    "We also set the `temperature` parameter to `1`, balancing creativity with coherence, ideal for writing engaging children's literature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script shows how to use the OpenAI API to generate text completions.\n",
    "We add the max_completion_tokens parameter to specify the number of tokens in the response.\n",
    "In this case, we want the response to be limited to one thousand tokens.\n",
    "The maximum number of tokens for gpt-4o-mini is 16,384.\n",
    "\"\"\"\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"developer\", \"content\": \"You are a brilliant author of children's books.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write two paragraphs about a frog.\"}\n",
    "  ],\n",
    "  response_format=None,  \n",
    "  temperature=None,\n",
    "  max_completion_tokens=1000 # Limit the number of tokens in the response, 16,384 tokens is the maximum for gpt-4o-mini\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of a Low `max_completion_tokens` Value on Responses\n",
    "\n",
    "This example highlights how setting the `max_completion_tokens` parameter to a very low value (`10` tokens) significantly constrains the length of the model's response. Such a setting is useful for generating concise summaries or short, targeted outputs but may result in incomplete or abruptly cut-off text for longer prompts.\n",
    "\n",
    "We've maintained a moderate `temperature` (`1`) to encourage creativity, but the output is heavily restricted by the token limit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script shows how to use the OpenAI API to generate text completions.\n",
    "We add the max_completion_tokens parameter to specify the number of tokens in the response.\n",
    "In this case, we want the response to be limited to ten tokens.\n",
    "The maximum number of tokens for gpt-4o-mini is 16,384.\n",
    "\"\"\"\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"developer\", \"content\": \"You are a brilliant author of children's books.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write two paragraphs about a frog.\"}\n",
    "  ],\n",
    "  response_format=None,  \n",
    "  temperature=None,\n",
    "  max_completion_tokens=10 # Limit the number of tokens in the response\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop\n",
    "\n",
    "### Controlling Response Generation with the `stop` Parameter\n",
    "\n",
    "In this example, we introduce the `stop` parameter to define explicit stopping conditions for the text generation. By setting `stop=[\"water\", \"green\"]`, we instruct the model to immediately end the response if either of these specified words appears in the generated text. This parameter is especially useful for controlling content boundaries or maintaining thematic coherence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script shows how to use the OpenAI API to generate text completions.\n",
    "We add the stop parameter to specify when token prediction should stop.\n",
    "In this case, we want the response to stop if \"water\" or \"green\" is encountered.\n",
    "\"\"\"\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"developer\", \"content\": \"You are a brilliant author of children's books.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write one page about a frog.\"}\n",
    "  ],\n",
    "  response_format=None,  \n",
    "  temperature=None,\n",
    "  max_completion_tokens=None, \n",
    "  stop=[\"water\",\"green\"] # Note: The stop parameter is a list of strings, and generation will stop when any of these strings are encountered in the output.\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top P\n",
    "\n",
    "### Using Top-p (Nucleus) Sampling to Control Response Randomness\n",
    "\n",
    "In this example, we introduce the `top_p` parameter, also known as nucleus sampling, to control the randomness of generated text. By setting `top_p` to a low value (`0.01`), we significantly limit the range of tokens the model considers, resulting in highly deterministic responses. A higher `top_p` value allows for more variability and creativity.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script shows how to use the OpenAI API to generate text completions.\n",
    "We add the top_p parameter to specify the randomness/creativity of the response.\n",
    "In this case, we want the response to be less random/creative.\n",
    "\"\"\"\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"developer\", \"content\": \"You are a brilliant author of children's books.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write two paragraphs about a frog.\"}\n",
    "  ],\n",
    "  response_format=None,  \n",
    "  temperature=None,\n",
    "  max_completion_tokens=None, \n",
    "  stop=None,\n",
    "  top_p=0.01, # Top-p sampling (nucleus sampling) to control randomness\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increasing Randomness with Higher Top-p Values\n",
    "\n",
    "In this example, we've set the `top_p` parameter (nucleus sampling) to a higher value (`0.90`). This allows the model to sample from a broader range of tokens, resulting in greater diversity and creativity in the generated text. Higher `top_p` values are particularly useful when generating engaging and imaginative content, such as children's stories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script shows how to use the OpenAI API to generate text completions.\n",
    "We add the top_p parameter to specify the randomness/creativity of the response.\n",
    "In this case, we want the response to be more random/creative.\n",
    "\"\"\"\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"developer\", \"content\": \"You are a brilliant author of children's books.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write two paragraphs about a frog.\"}\n",
    "  ],\n",
    "  response_format=None,  \n",
    "  temperature=None,\n",
    "  max_completion_tokens=None, \n",
    "  stop=None,\n",
    "  top_p=0.90, # Top-p sampling (nucleus sampling) to control randomness\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Penalty\n",
    "\n",
    "### Reducing Token Repetition with Frequency Penalty\n",
    "\n",
    "In this example, we introduce the `frequency_penalty` parameter to discourage the model from repeatedly using the same tokens or phrases. By setting `frequency_penalty` to a higher value (`1.5`), we prompt the model to diversify its vocabulary, resulting in richer and more varied responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script shows how to use the OpenAI API to generate text completions.\n",
    "We add the frequency_penalty parameter to avoid repeated tokens.\n",
    "In this case, we want the response to allow fewer repeated tokens.\n",
    "\"\"\"\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"developer\", \"content\": \"You are a brilliant author of children's books.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write two paragraphs about a frog.\"}\n",
    "  ],\n",
    "  response_format=None,  \n",
    "  temperature=None,\n",
    "  max_completion_tokens=None, \n",
    "  stop=None,\n",
    "  top_p=None, \n",
    "  frequency_penalty=1.5, # Frequency penalty to control repetition\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allowing More Repetition with Lower Frequency Penalty\n",
    "\n",
    "In this example, we've set the `frequency_penalty` parameter to a lower value (`0.05`), making the model less restricted regarding repeated tokens or phrases. This approach can be helpful in scenarios where natural repetition, rhythm, or emphasis contributes positively to the readability and engagement of the content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script shows how to use the OpenAI API to generate text completions.\n",
    "We add the frequency_penalty parameter to avoid repeated tokens.\n",
    "In this case, we want the response to allow more repeated tokens.\n",
    "\"\"\"\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"developer\", \"content\": \"You are a brilliant author of children's books.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write two paragraphs about a frog.\"}\n",
    "  ],\n",
    "  response_format=None,  \n",
    "  temperature=None,\n",
    "  max_completion_tokens=None, \n",
    "  stop=None,\n",
    "  top_p=None, \n",
    "  frequency_penalty=0.05, # Frequency penalty to control repetition\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presence Penalty\n",
    "\n",
    "### Controlling Token Repetition with Presence Penalty\n",
    "\n",
    "In this example, we introduce the `presence_penalty` parameter to discourage the reuse of topics or tokens that have already appeared in the generated text. Setting the `presence_penalty` to a higher value (`1.5`) encourages the model to introduce new concepts or words, resulting in more diverse and less repetitive responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script shows how to use the OpenAI API to generate text completions.\n",
    "We add the presence_penalty parameter to avoid repeated tokens.\n",
    "In this case, we want the response to allow fewer repeated tokens.\n",
    "\"\"\"\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"developer\", \"content\": \"You are a brilliant author of children's books.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write two paragraphs about a frog.\"}\n",
    "  ],\n",
    "  response_format=None,  \n",
    "  temperature=None,\n",
    "  max_completion_tokens=None, \n",
    "  stop=None,\n",
    "  top_p=None, \n",
    "  frequency_penalty=None,\n",
    "  presence_penalty=1.5 # Presence penalty to control repetition\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allowing More Repetition with Lower Presence Penalty\n",
    "\n",
    "In this example, we set the `presence_penalty` parameter to a low value (`0.05`). This setting allows the model greater flexibility to repeat previously mentioned topics or tokens, which can be useful when repetition contributes positively to the natural flow, style, or emphasis of the generated content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script shows how to use the OpenAI API to generate text completions.\n",
    "We add the presence_penalty parameter to avoid repeated tokens.\n",
    "In this case, we want the response to allow more repeated tokens.\n",
    "\"\"\"\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"developer\", \"content\": \"You are a brilliant author of children's books.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write two paragraphs about a frog.\"}\n",
    "  ],\n",
    "  response_format=None,  \n",
    "  temperature=None,\n",
    "  max_completion_tokens=None, \n",
    "  stop=None,\n",
    "  top_p=None, \n",
    "  frequency_penalty=None,\n",
    "  presence_penalty=0.05 # Presence penalty to control repetition\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming\n",
    "\n",
    "### Real-Time Responses Using the Stream Parameter\n",
    "\n",
    "In this example, we introduce the `stream` parameter (`stream=True`) to enable real-time streaming of the model's output. Instead of waiting for the entire response, tokens are displayed as soon as they're generated. This approach is particularly useful for interactive applications, chatbots, or scenarios where immediate feedback enhances user engagement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script shows how to use the OpenAI API to generate text completions.\n",
    "We add the stream parameter to dynamically show tokens to the user in real-time.\n",
    "In this case, we want the response to start showing as soon as possible.\n",
    "\"\"\"\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"developer\", \"content\": \"You are a brilliant author of children's books.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write two paragraphs about a frog.\"}\n",
    "    ],\n",
    "    response_format=None,  \n",
    "    temperature=None,\n",
    "    max_completion_tokens=None, \n",
    "    stop=None,\n",
    "    top_p=None, \n",
    "    frequency_penalty=None,\n",
    "    presence_penalty=None,\n",
    "    stream=True # Enable streaming\n",
    "    )\n",
    "\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receiving Complete Responses with Streaming Disabled\n",
    "\n",
    "In this example, we've set `stream=False` to disable real-time token streaming. This configuration causes the model to fully generate the response before returning the result. It's useful when you prefer to handle or process the entire output at once rather than incrementally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script shows how to use the OpenAI API to generate text completions.\n",
    "We add the stream parameter to dynamically show tokens to the user in real-time.\n",
    "In this case, we want the response to delay showing the response until it is complete.\n",
    "\"\"\"\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"developer\", \"content\": \"You are a brilliant author of children's books.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write two paragraphs about a frog.\"}\n",
    "    ],\n",
    "    response_format=None,  \n",
    "    temperature=None,\n",
    "    max_completion_tokens=None, \n",
    "    stop=None,\n",
    "    top_p=None, \n",
    "    frequency_penalty=None,\n",
    "    presence_penalty=None,\n",
    "    stream=False\n",
    "    )\n",
    "\n",
    "print(stream.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai_api_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
